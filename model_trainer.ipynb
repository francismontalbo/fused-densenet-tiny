{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DEPENDENCIES\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import applications\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.models import Model, Input\n",
    "from sklearn.utils import class_weight\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.densenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Layer, ReLU, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Add, Concatenate, Dropout\n",
    "\n",
    "#PREVENT ERROR UNCESSARY MESSAGES\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD THE DATA\n",
    "train_data_dir = \"data/train/\"\n",
    "validation_data_dir = \"data/validation/\"\n",
    "\n",
    "# THE INPUT LAYER IS THE SAME AS IT WILL BE FUSED AS ONE LATER ON\n",
    "img_rows, img_cols = 224, 224\n",
    "input_shape = (img_rows,img_cols,3)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(\"Data folders found!\")\n",
    "print(\"The Input size is set to \", model_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA GENERATORS\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "                                \n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows,img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "         classes=['0_Normal', '1_Covid19', '2_Pneumonia'])\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows,img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False,\n",
    "         classes=['0_Normal', '1_Covid19', '2_Pneumonia'])\n",
    "\n",
    "#CHECK  THE NUMBER OF SAMPLES\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "\n",
    "if nb_train_samples == 0:\n",
    "    print(\"NO DATA TRAIN FOUND! Please check your train data path and folders!\")\n",
    "else:\n",
    "    print(\"Train samples found!\")\n",
    "    \n",
    "if nb_validation_samples == 0:\n",
    "    print(\"NO DATA VALIDATION FOUND! Please check your validation data path and folders!\")\n",
    "    print(\"Check the data folders first!\")\n",
    "else:\n",
    "    print(\"Validation samples found!\")\n",
    "\n",
    "#check the class indices\n",
    "train_generator.class_indices\n",
    "validation_generator.class_indices\n",
    "\n",
    "#true labels\n",
    "Y_test=validation_generator.classes\n",
    "print(Y_test)\n",
    "\n",
    "num_classes= len(train_generator.class_indices)\n",
    "\n",
    "if nb_train_samples and nb_validation_samples > 0:\n",
    "    print(\"Generators are set!\")\n",
    "    print(\"Check if dataset is complete and has no problems before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight each class\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "print(class_weights)\n",
    "\n",
    "if class_weights != class_weights:\n",
    "    print(\"Data imbalance detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet121-A \n",
    "\n",
    "#TRANSFER LEARNING\n",
    "def densenet_tiny_A_builder(model_input):\n",
    "    densenet_tiny_A_builder = DenseNet121(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    \n",
    "#Partial LAYER FREEZING\n",
    "    for layer in densenet_tiny_A_builder.layers:\n",
    "        layer.trainable = False \n",
    "        \n",
    "    x = densenet_tiny_A_builder.layers[-354].output\n",
    "    model = Model(inputs=densenet_tiny_A_builder.input, outputs=x, name='densenet-tiny-A')\n",
    "    return model\n",
    "\n",
    "#GENERATE THE MODEL\n",
    "densenet_tiny_A = densenet_tiny_A_builder(model_input)\n",
    "\n",
    "#PLOT THE MODEL STRUCTURE\n",
    "densenet_tiny_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet121-B\n",
    "\n",
    "#TRANSFER LEARNING\n",
    "def densenet_tiny_B_builder(model_input):\n",
    "    densenet_tiny_B_builder = DenseNet121(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    \n",
    "#RE-TRAINING ALL LAYERS (RE-NAMING LAYERS TO PREVENT OVERLAPS)\n",
    "    for layer in densenet_tiny_B_builder.layers:\n",
    "        layer.trainable = True\n",
    "        layer.name = layer.name + str(\"_mirror\")\n",
    "        \n",
    "    x = densenet_tiny_B_builder.layers[-354].output\n",
    "    model = Model(inputs=densenet_tiny_B_builder.input, outputs=x, name='densenet_tiny-B')\n",
    "    return model\n",
    "\n",
    "#GENERATE THE MODEL\n",
    "densenet_tiny_B = densenet_tiny_B_builder(model_input)\n",
    "\n",
    "#PLOT THE MODEL STRUCTURE\n",
    "densenet_tiny_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARE THE CONCATENATION OF THE PRE-TRAINED MODELS\n",
    "densenet_tiny_A = densenet_tiny_A_builder(model_input)\n",
    "densenet_tiny_B = densenet_tiny_B_builder(model_input)\n",
    "\n",
    "print(\"DenseNet-Tiny-A and DenseNet-Tiny-B accomplished Pre-training and ready for concatenation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONCATENATE AS A SINGLE PIPELINE\n",
    "\n",
    "models = [densenet_tiny_A, \n",
    "          densenet_tiny_B]\n",
    "\n",
    "print(\"Concatenation success!\")\n",
    "print(\"Fused-DenseNet-Tiny ready to connect with its ending layers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble model definition is very straightforward. It uses the same input layer thas is shared between all previous models. \n",
    "In the top layer, the ensemble computes the average of three models' outputs (predictions) by using Average() layer. The ensemble is expected to have a lower error rate than any single model and better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILD THE FUSED-DENSENET-TINY\n",
    "\n",
    "def fused_densenet_tiny(models, model_input):\n",
    "    outputs = [m.output for m in models]\n",
    "    y = Add()(outputs)               \n",
    "    y = GlobalAveragePooling2D()(y)\n",
    "    y = Dense(512, activation='relu', use_bias=True)(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    prediction = Dense(num_classes,activation='softmax', name='Softmax_Classifier')(y)\n",
    "    model = Model(model_input, prediction, name='fused_densenet_tiny')\n",
    "    return model\n",
    "\n",
    "#istantitate the ensemble model and report the summary\n",
    "fused_densenet_tiny = fused_densenet_tiny(models,model_input)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"Fused-DenseNet-Tiny complete and ready for compilation and training!\")\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "fused_densenet_tiny.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL COMPILATION WITH HYPER-PARAMETERS, LOSS FUNCTIONS AND TRAINING!\n",
    "import time\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "optimizer = Adam(lr=0.0001)\n",
    "\n",
    "fused_densenet_tiny.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2,\n",
    "                              verbose=1, mode='max', min_lr=0.000001)\n",
    "\n",
    "callbacks = [reduce_lr]\n",
    "\n",
    "history = fused_densenet_tiny.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  callbacks=callbacks, \n",
    "                                  validation_steps=nb_validation_samples // batch_size, verbose=1)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE THE FUSED-DENSENET-TINY\n",
    "\n",
    "fused_densenet_tiny.save('weights/fused_densenet_tiny.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE THE HISTORY FOR EVALUATION\n",
    "\n",
    "from pickle import dump\n",
    "dump(history, open('history/fused_densenet_tiny.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
